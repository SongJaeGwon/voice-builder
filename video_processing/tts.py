from video_processing import os, AudioSegment, ElevenLabs, subprocess, torchaudio, torch, requests, parse_srt, get_file_path
from config import ELEVENLABS_API_KEY

def extract_speech_with_elevenlabs(input_audio, output_audio):
    output_path = get_file_path(output_audio)

    client = ElevenLabs(api_key=ELEVENLABS_API_KEY)

    # ğŸ™ï¸ Step 1: ElevenLabs API í˜¸ì¶œ (ìŒì„±ë§Œ ë¶„ë¦¬)
    with open(input_audio, "rb") as audio_file:
        audio = client.audio_isolation.audio_isolation(audio=audio_file)
        audio_bytes = b"".join(chunk for chunk in audio)

    temp_audio_path = output_path.replace(".wav", "_temp.wav")

    # ğŸ”½ Step 2: APIì—ì„œ ë°›ì€ ì˜¤ë””ì˜¤ ì €ì¥ (ì„ì‹œ íŒŒì¼)
    with open(temp_audio_path, "wb") as f:
        f.write(audio_bytes)

    # ğŸ”„ Step 3: ì˜¤ë””ì˜¤ë¥¼ 16kHz ìƒ˜í”Œë§ ì†ë„ë¡œ ë³€í™˜
    waveform, sample_rate = torchaudio.load(temp_audio_path)

    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
        waveform = resampler(waveform)

    # ğŸ› ï¸ Step 4: ê¸¸ì´ê°€ ë‹¤ë¥¸ ì˜¤ë””ì˜¤ ì¡°ê° ë§ì¶”ê¸° (Padding)
    def pad_audio(waveform, target_length=160000):
        """ì˜¤ë””ì˜¤ë¥¼ íŠ¹ì • ê¸¸ì´ë¡œ íŒ¨ë”©"""
        current_length = waveform.shape[1]  # ì±„ë„ x ìƒ˜í”Œ ìˆ˜
        if current_length < target_length:
            pad_size = target_length - current_length
            waveform = torch.nn.functional.pad(waveform, (0, pad_size), mode="constant", value=0)
        return waveform

    padded_waveform = pad_audio(waveform)

    # ğŸ§ Step 5: ìµœì¢… ì˜¤ë””ì˜¤ ì €ì¥
    torchaudio.save(output_path, padded_waveform, 16000)

    # ğŸ”„ Step 6: ì„ì‹œ íŒŒì¼ ì‚­ì œ
    os.remove(temp_audio_path)

    return output_path

def generate_tts_with_timestamps(srt_file, speaker_voice_id_list, filename="tts_audio.mp3"):
    output_path = get_file_path(filename)
    subtitles = parse_srt(srt_file)
    combined_audio = AudioSegment.silent(duration=0)
    previous_ids = []

    speaker_voice_map = convert_list_to_speaker_map(speaker_voice_id_list)

    for idx, subtitle in enumerate(subtitles):
        text = subtitle["text"]
        speaker = subtitle["speaker"]
        start_ms = int(subtitle["start"] * 1000)
        end_ms = int(subtitle["end"] * 1000)
        duration_ms = end_ms - start_ms

        # voice_id = SPEAKER_VOICE_MAP.get(speaker, default_voice_id)  # ê¸°ë³¸ê°’ ì ìš©
        voice_id = speaker_voice_map.get(speaker, speaker_voice_id_list[0])  # ê¸°ë³¸ê°’ ì ìš©

        temp_tts_file = f"temp_{idx}.mp3"
        
        request_id = generate_speech_with_elevenlabs(text, voice_id, temp_tts_file, previous_ids)
        if request_id:
            previous_ids.append(request_id)  # ìƒˆ ID ì¶”ê°€
            previous_ids = previous_ids[-3:]  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ìœ ì§€

        tts_audio = AudioSegment.from_file(temp_tts_file)
        tts_duration = len(tts_audio)

        print(f"ğŸ“Œ [{idx}] ìŒì„± íŒŒì¼ ìƒì„±")
        print(f"   â–¶ ì›ë³¸ SRT íƒ€ì„ìŠ¤íƒ¬í”„: {subtitle['start']}s ~ {subtitle['end']}s ({duration_ms}ms)")
        print(f"   â–¶ ìƒì„±ëœ ìŒì„± ê¸¸ì´: {tts_duration / 1000:.2f}s")

        if tts_duration > duration_ms:
            speed_factor = tts_duration / duration_ms
            adjusted_tts_file = f"adjusted_{idx}.mp3"

            # FFmpegì„ ì´ìš©í•´ ë°°ì† ì¡°ì •
            adjust_audio_speed(temp_tts_file, adjusted_tts_file, speed_factor)

            print(f"   â–¶ ê¸¸ì´ ì´ˆê³¼ â†’ {speed_factor:.2f}ë°°ì† ì ìš©")

            # ë³€í™˜ëœ ì˜¤ë””ì˜¤ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°
            tts_audio = AudioSegment.from_file(adjusted_tts_file)
            os.remove(adjusted_tts_file)
        elif tts_duration < duration_ms:
            silence = AudioSegment.silent(duration=duration_ms - tts_duration)
            tts_audio = tts_audio + silence

        audio_start = start_ms
        audio_end = audio_start + len(tts_audio)

        if start_ms > len(combined_audio):
            gap = AudioSegment.silent(duration=audio_start - len(combined_audio))
            combined_audio += gap

        combined_audio += tts_audio

        print(f"   ğŸ” ì‹¤ì œ ìŒì„± íŒŒì¼ íƒ€ì„ìŠ¤íƒ¬í”„: {audio_start / 1000:.2f}s ~ {audio_end / 1000:.2f}s")

        os.remove(temp_tts_file)

    combined_audio.export(output_path, format="mp3")

    return output_path

def generate_speech_with_elevenlabs(text, voice_id, output_audio, previous_request_ids=None):

    # ìš”ì²­ ë³¸ë¬¸ êµ¬ì„±
    request_data = {
        "voice_id": voice_id,
        "output_format": "mp3_44100_128",
        "text": text,
        "model_id": "eleven_multilingual_v2"
    }

    if previous_request_ids:
        request_data["previous_request_ids"] = previous_request_ids

    # API ìš”ì²­
    url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
    headers = {
        "xi-api-key": ELEVENLABS_API_KEY,
        "Content-Type": "application/json"
    }

    response = requests.post(url, headers=headers, json=request_data)

    if response.status_code == 200:
        audio_bytes = response.content

        # ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥
        with open(output_audio, "wb") as f:
            f.write(audio_bytes)

        # ì‘ë‹µ í—¤ë”ì—ì„œ request_id ì¶”ì¶œ
        request_id = response.headers.get("request-id")

        return request_id
    else:
        print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {response.status_code} - {response.text}")
        return None

def adjust_audio_speed(input_audio, output_audio, speed_factor):
    """
    FFmpegì„ ì‚¬ìš©í•˜ì—¬ ì†ë„ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°ì • (rubberband í•„í„° ì ìš©)
    """
    command = [
        "ffmpeg", "-i", input_audio,
        "-filter:a", f"rubberband=pitch=1.0:tempo={speed_factor}",
        "-vn", output_audio, "-loglevel", "error", "-y"
    ]
    subprocess.run(command, check=True)


def convert_list_to_speaker_map(voice_id_list):
    """
    voice_id_listì˜ ê° ì›ì†Œë¥¼ SPEAKER_XX í˜•ì‹ì˜ keyì— í• ë‹¹í•˜ì—¬ dictionaryë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì˜ˆ: ['id0', 'id1', 'id2', 'id3', 'id4'] ->
        {
            "SPEAKER_00": "id0",
            "SPEAKER_01": "id1",
            "SPEAKER_02": "id2",
            "SPEAKER_03": "id3",
            "SPEAKER_04": "id4",
        }
    """
    return {f"SPEAKER_{i:02}": voice_id for i, voice_id in enumerate(voice_id_list)}