from video_processing import json, openai, SpeakerDiarization, get_file_path
from config import OPEN_AI_TOKEN, HF_TOKEN

diarization_pipeline = SpeakerDiarization.from_pretrained(
    "pyannote/speaker-diarization-3.1",
    use_auth_token=HF_TOKEN
)

def transcribe_audio_whisper(audio_file, num_speakers=None, model="whisper-1"):
    client = openai.OpenAI(api_key=OPEN_AI_TOKEN)

    # ğŸ”¹ OpenAI Whisper API ìš”ì²­
    with open(audio_file, "rb") as file:
        response = client.audio.transcriptions.create(
            model=model,
            file=file,
            response_format="verbose_json",
            timestamp_granularities=["segment", "word"],
        )

    response_json = response.model_dump()

    with open(get_file_path("transcription_whisper.json"), "w", encoding="utf-8") as f:
        json.dump(response_json, f, indent=4, ensure_ascii=False)

    # ğŸ”¹ í™”ì ë¶„ë¦¬ ìˆ˜í–‰ (Pyannote)
    diarization_result = diarize_audio(audio_file, num_speakers=num_speakers)

    # ğŸ”¹ í™”ì ì •ë³´ì™€ Whisper í…ìŠ¤íŠ¸ ë§¤ì¹­
    speaker_segments = match_speakers_with_transcription(diarization_result, response_json)

    output_path = get_file_path("transcription_whisper_speaker.json")
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(speaker_segments, f, indent=4, ensure_ascii=False)

    return speaker_segments

def diarize_audio(audio_file, num_speakers=None):
    """Pyannoteë¥¼ ì‚¬ìš©í•˜ì—¬ í™”ì ë¶„ë¦¬ ìˆ˜í–‰"""

    # ğŸ”¹ `num_speakers`ê°€ ì§€ì •ëœ ê²½ìš°, í•´ë‹¹ ê°’ìœ¼ë¡œ ì„¤ì •
    params = {"num_speakers": num_speakers} if num_speakers else {}

    diarization_result = diarization_pipeline({"uri": "audio", "audio": audio_file}, **params)
    
    speaker_timestamps = []
    for speech_turn, track, speaker in diarization_result.itertracks(yield_label=True):
        speaker_timestamps.append({
            "speaker": speaker,
            "start": speech_turn.start,
            "end": speech_turn.end
        })

    return speaker_timestamps

def match_speakers_with_transcription(
    diarization_result, whisper_response, fill_nearest=False
):
    """
    Whisperì˜ í…ìŠ¤íŠ¸ì™€ Pyannote í™”ì ë¶„ë¦¬ ê²°ê³¼ë¥¼ ë§¤ì¹­í•˜ì—¬ 'segments' ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    - ë¬¸ì¥(Sentence) ë‹¨ìœ„ ë° ë‹¨ì–´(Word) ë‹¨ìœ„ í™”ì ì •ë³´ ì¶”ê°€
    - Whisper ì‘ë‹µì—ì„œ 'segments'ì™€ 'words'ê°€ ë”°ë¡œ ì œê³µë˜ë¯€ë¡œ ì´ë¥¼ ë³‘í•©
    - `fill_nearest=True` ì‹œ ê°€ì¥ ê°€ê¹Œìš´ í™”ìë¥¼ ìë™ìœ¼ë¡œ í• ë‹¹
    """
    whisper_segments = whisper_response.get("segments", [])
    whisper_words = whisper_response.get("words", [])

    segments = []

    for seg in whisper_segments:
        start_time = seg["start"]
        end_time = seg["end"]
        text = seg["text"]

        # ğŸ”¹ í™”ì êµ¬ê°„ê³¼ Whisper êµ¬ê°„ì˜ êµì§‘í•© ê³„ì‚° (intersection)
        diarization_intersections = [
            {
                "speaker": diarization["speaker"],
                "intersection": max(
                    0, min(diarization["end"], end_time) - max(diarization["start"], start_time)
                )
            }
            for diarization in diarization_result
        ]

        # ğŸ”¹ ê°€ì¥ ë§ì´ ê²¹ì¹˜ëŠ” í™”ì ì°¾ê¸°
        if diarization_intersections:
            best_match = max(diarization_intersections, key=lambda x: x["intersection"])
            speaker = best_match["speaker"] if best_match["intersection"] > 0 else "Unknown"
        else:
            speaker = "Unknown"

        # ğŸ”¹ í™”ì ì •ë³´ê°€ ì—†ì„ ê²½ìš°, `fill_nearest` ì˜µì…˜ì— ë”°ë¼ ì²˜ë¦¬
        if speaker == "Unknown" and fill_nearest:
            closest_speaker = min(
                diarization_result,
                key=lambda x: abs(x["start"] - start_time),
                default=None,
            )
            if closest_speaker:
                speaker = closest_speaker["speaker"]

        # ğŸ”¹ í˜„ì¬ `segment`ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ ì°¾ê¸°
        words = []
        for word in whisper_words:
            word_start = word.get("start")
            word_end = word.get("end")

            # ë‹¨ì–´ê°€ í˜„ì¬ segment ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
            if word_start and word_end and start_time <= word_start <= end_time:
                diarization_intersections = [
                    {
                        "speaker": diarization["speaker"],
                        "intersection": max(
                            0, min(diarization["end"], word_end) - max(diarization["start"], word_start)
                        )
                    }
                    for diarization in diarization_result
                ]

                # ğŸ”¹ ë‹¨ì–´ë³„ ê°€ì¥ ë§ì´ ê²¹ì¹˜ëŠ” í™”ì ì°¾ê¸°
                if diarization_intersections:
                    best_match = max(diarization_intersections, key=lambda x: x["intersection"])
                    word_speaker = best_match["speaker"] if best_match["intersection"] > 0 else speaker
                else:
                    word_speaker = speaker

                words.append({
                    "word": word["word"],
                    "start": word_start,
                    "end": word_end,
                    "speaker": word_speaker
                })

        segments.append({
            "speaker": speaker,
            "start": start_time,
            "end": end_time,
            "text": text,
            "words": words
        })

    return {"segments": segments}

def refine_srt_with_gpt(srt_file_path, output_srt_path):
    with open(srt_file_path, "r", encoding="utf-8") as file:
        srt_content = file.read()

    prompt = f"""
    ë‹¤ìŒì€ ìë§‰ íŒŒì¼ì˜ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ìë§‰ì„ ë” ìì—°ìŠ¤ëŸ½ê³  ë¬¸ë§¥ì— ë§ê²Œ ìˆ˜ì •í•´ ì£¼ì„¸ìš”.
    ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³ , ë” ì½ê¸° ì‰½ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ ì£¼ì„¸ìš”.

    **SRT í˜•ì‹ ìœ ì§€ (ë²ˆí˜¸, íƒ€ì„ìŠ¤íƒ¬í”„, í…ìŠ¤íŠ¸ë§Œ í¬í•¨)**  
    **ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸(ì˜ˆ: ì½”ë“œ ë¸”ë¡, ```plaintext ë“±)ëŠ” í¬í•¨í•˜ì§€ ë§ ê²ƒ**  
    **ìì—°ìŠ¤ëŸ½ê³  ê°€ë…ì„± ì¢‹ì€ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜**

    -- ì›ë³¸ ìë§‰ --
    {srt_content}

    -- ë³€í™˜ëœ ìë§‰ --
    """

    client = openai.OpenAI(api_key=OPEN_AI_TOKEN)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": "ë‹¹ì‹ ì€ ìë§‰ì„ ë‹¤ë“¬ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                  {"role": "user", "content": prompt}],
        temperature=0.7
    )

    refined_srt = response.choices[0].message.content.strip()

    with open(output_srt_path, "w", encoding="utf-8") as file:
        file.write(refined_srt)

    return output_srt_path