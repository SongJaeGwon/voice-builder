import os
import re
import yt_dlp
import subprocess
import requests
import whisper
from elevenlabs.client import ElevenLabs
from pydub import AudioSegment
from dotenv import load_dotenv

# ğŸ”¹ .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ğŸ”¹ í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")
DEEPL_API_KEY = os.getenv("DEEPL_API_KEY")
API_URL = os.getenv("API_URL")

video_url = "https://www.youtube.com/watch?v=nm6wzLMphh4"  # ë³€í™˜í•  ìœ íŠœë¸Œ ì˜ìƒ ë§í¬
target_language = "JA"  # ë²ˆì—­í•  ì–¸ì–´ (ì˜ˆ: ì¼ë³¸ì–´)
voice_id = "aAi5am3XBah3myZdv6ON"  # ElevenLabsì—ì„œ í•™ìŠµí•œ ëª©ì†Œë¦¬ ID ì…ë ¥

def download_youtube_video(video_url, output_filename="downloaded_video.mp4"):
    ydl_opts = {
        'format': 'bestvideo+bestaudio/best',
        'outtmpl': output_filename,
        'merge_output_format': 'mp4'
    }
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.download([video_url])
    return output_filename

def trim_video(input_file, output_file="trimmed_video.mp4", start_time="00:00:30", duration="29"):
    command = f"ffmpeg -i {input_file} -ss {start_time} -t {duration} -c:v copy -c:a copy {output_file} -y"
    subprocess.run(command, shell=True)
    return output_file

def extract_audio_from_video(video_file, audio_output="audio.mp3"):
    command = f"ffmpeg -i {video_file} -q:a 0 -map a {audio_output} -y"
    subprocess.run(command, shell=True)
    return audio_output

def transcribe_audio_whisper(audio_file, model_size="medium"):
    """
    OpenAI Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  SRT íŒŒì¼ì„ ìƒì„±
    """
    # ğŸ”¹ Whisper ëª¨ë¸ ë¡œë“œ
    model = whisper.load_model(model_size)

    # ğŸ”¹ ì˜¤ë””ì˜¤ ë³€í™˜ (STT ìˆ˜í–‰)
    result = model.transcribe(audio_file, language="ko", word_timestamps=True)

    return result

def seconds_to_srt_time(seconds):
    """ ì´ˆ ë‹¨ìœ„ë¥¼ SRT í˜•ì‹ (HH:MM:SS,mmm)ìœ¼ë¡œ ë³€í™˜ """
    millisec = int((seconds % 1) * 1000)
    seconds = int(seconds)
    hours, remainder = divmod(seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    return f"{hours:02}:{minutes:02}:{seconds:02},{millisec:03}"

def create_srt(transcription, output_srt="output.srt"):
    """
    Whisper ë³€í™˜ ê²°ê³¼ë¥¼ SRT íŒŒì¼ë¡œ ì €ì¥
    """
    if "segments" not in transcription:
        print("âš ï¸ ë³€í™˜ëœ ìë§‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    with open(output_srt, "w", encoding="utf-8") as f:
        for idx, segment in enumerate(transcription["segments"]):
            start_time = seconds_to_srt_time(segment["start"])
            end_time = seconds_to_srt_time(segment["end"])
            text = segment["text"]

            f.write(f"{idx+1}\n{start_time} --> {end_time}\n{text}\n\n")

    print(f"âœ… SRT íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_srt}")

def translate_text(text, src_lang="KO", target_lang="JA"):
    """
    DeepL APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë²ˆì—­
    :param text: ë²ˆì—­í•  ì›ë³¸ í…ìŠ¤íŠ¸
    :param src_lang: ì›ë³¸ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: "KO" -> í•œêµ­ì–´)
    :param target_lang: ëª©í‘œ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: "JA" -> ì¼ë³¸ì–´)
    :return: ë²ˆì—­ëœ í…ìŠ¤íŠ¸
    """
    headers = {"Authorization": f"DeepL-Auth-Key {DEEPL_API_KEY}"}
    data = {
        "text": [text],  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì „ë‹¬í•´ì•¼ í•¨
        "source_lang": src_lang.upper(),  # DeepLì€ ëŒ€ë¬¸ì ì–¸ì–´ ì½”ë“œ ì‚¬ìš©
        "target_lang": target_lang.upper()
    }

    response = requests.post(API_URL, headers=headers, data=data)

    if response.status_code == 200:
        result = response.json()
        return result["translations"][0]["text"]  # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ë°˜í™˜
    else:
        print("âŒ ë²ˆì—­ ì‹¤íŒ¨:", response.text)
        return None

def translate_srt(input_srt, output_srt, src_lang="KO", target_lang="JA"):
    """
    DeepL APIë¥¼ ì‚¬ìš©í•˜ì—¬ SRT íŒŒì¼ì„ ë²ˆì—­í•˜ì—¬ ìƒˆë¡œìš´ SRT íŒŒì¼ ìƒì„±
    """
    with open(input_srt, "r", encoding="utf-8") as f:
        lines = f.readlines()

    translated_lines = []
    for line in lines:
        if "-->" in line or line.strip().isdigit() or line.strip() == "":
            translated_lines.append(line)  # ì‹œê°„ ë° ì¸ë±ìŠ¤ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        else:
            translated_text = translate_text(line.strip(), src_lang, target_lang)
            translated_lines.append(translated_text + "\n")

    with open(output_srt, "w", encoding="utf-8") as f:
        f.writelines(translated_lines)

    print(f"âœ… ë²ˆì—­ ì™„ë£Œ! ìƒˆë¡œìš´ SRT íŒŒì¼ ì €ì¥: {output_srt}")
    return output_srt

# ğŸ”¹ Step 5: ElevenLabs APIë¥¼ ì´ìš©í•´ ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
def generate_speech_with_elevenlabs(text, voice_id, output_audio):
    api_key = "sk_8b6c5b223753891ae028b53efcacf995902fa09bcbde2b55"
    client = ElevenLabs(api_key=api_key)

    # TTS ë³€í™˜ ì‹¤í–‰
    audio = client.text_to_speech.convert(
        voice_id=voice_id,
        output_format="mp3_44100_128",
        text=text,
        model_id="eleven_multilingual_v2"
    )

    # ğŸ”¹ ì œë„ˆë ˆì´í„° ë°ì´í„°ë¥¼ ë°”ì´íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    audio_bytes = b"".join(audio)  

    # ë³€í™˜ëœ ì˜¤ë””ì˜¤ ì €ì¥
    with open(output_audio, "wb") as f:
        f.write(audio_bytes)

def parse_srt(srt_file):
    """
    SRT íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë° í…ìŠ¤íŠ¸ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì¶”ì¶œ
    """
    srt_pattern = re.compile(r"(\d+)\s*\n(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})\s*\n([\s\S]*?)(?=\n\d+\s*\n|\Z)")

    with open(srt_file, "r", encoding="utf-8") as file:
        content = file.read()

    matches = srt_pattern.findall(content)
    subtitles = []

    for _, start, end, text in matches:
        start_seconds = srt_time_to_seconds(start)
        end_seconds = srt_time_to_seconds(end)

        # ğŸ”¹ ì—¬ëŸ¬ ì¤„ì˜ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì¤„ë¡œ í•©ì¹˜ê¸°
        text = text.strip().replace("\n", " ")

        subtitles.append({"start": start_seconds, "end": end_seconds, "text": text})

    return subtitles

def srt_time_to_seconds(time_str):
    """SRT ì‹œê°„ ë¬¸ìì—´ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜ (HH:MM:SS,mmm â†’ ì´ˆ)"""
    hours, minutes, seconds_millis = time_str.split(":")
    seconds, millis = map(int, seconds_millis.split(","))
    return int(hours) * 3600 + int(minutes) * 60 + seconds + millis / 1000

def generate_tts_with_timestamps(srt_file, voice_id, output_audio="final_tts_audio.mp3"):
    """
    ë²ˆì—­ëœ SRT íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ, íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ê³ ë ¤í•œ ìŒì„±ì„ ìƒì„±í•˜ê³  í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë³‘í•©
    """
    subtitles = parse_srt(srt_file)
    final_audio = AudioSegment.silent(duration=int(subtitles[-1]["end"] * 1000))  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤íƒ¬í”„ê¹Œì§€ ë¹ˆ ì˜¤ë””ì˜¤ ìƒì„±
    
    for idx, subtitle in enumerate(subtitles):
        text = subtitle["text"]
        start_time = int(subtitle["start"] * 1000)  # ë°€ë¦¬ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
        tts_filename = f"temp_{idx}.mp3"

        # ğŸ”¹ ElevenLabs TTS ìƒì„±
        generate_speech_with_elevenlabs(text, voice_id, tts_filename)
        
        # ğŸ”¹ TTS ì˜¤ë””ì˜¤ ë¡œë“œ
        tts_audio = AudioSegment.from_file(tts_filename)
        
        # ğŸ”¹ íƒ€ì„ìŠ¤íƒ¬í”„ì— ë§ê²Œ ë°°ì¹˜
        final_audio = final_audio.overlay(tts_audio, position=start_time)

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(tts_filename)

    # ğŸ”¹ ìµœì¢… ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥
    final_audio.export(output_audio, format="mp3")
    print(f"âœ… íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ê³ ë ¤ëœ ìŒì„± íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_audio}")
    return output_audio

# ğŸ”¹ Step 6: ìƒˆ ìŒì„±ì„ ì›ë³¸ ì˜ìƒì— í•©ì¹˜ê¸°
def merge_audio_with_video(original_video, new_audio, output_video="final_video.mp4"):
    command = f"ffmpeg -i {original_video} -i {new_audio} -c:v copy -map 0:v:0 -map 1:a:0 -shortest {output_video} -y"
    subprocess.run(command, shell=True)
    return output_video

# ğŸ”¹ Step 7: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
def process_video(video_url, target_language="JA", voice_id="YOUR_VOICE_ID"):
    print("ğŸ“¥ 1. ìœ íŠœë¸Œ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    video_file = download_youtube_video(video_url)

    print("âœ‚ï¸ FFmpegë¡œ 30ì´ˆ ê¸¸ì´ë¡œ ìë¥´ê¸°...")
    trimmed_video = trim_video(video_file)

    print("ğŸ™ï¸ 2. ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘...")
    audio_file = extract_audio_from_video(trimmed_video)

    print("ğŸ“ 3. ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...")
    original_text = transcribe_audio_whisper(audio_file)

    print("ğŸ“ 4. SRT íŒŒì¼ ìƒì„± ì¤‘...")
    create_srt(original_text, "transcription.srt")

    print(f"ğŸŒ 5. ë²ˆì—­ ì¤‘... (ì–¸ì–´: {target_language})")
    translated_srt = translate_srt("transcription.srt", "translated.srt", src_lang="KO", target_lang=target_language)

    if translated_srt is None:
        print("âŒ ë²ˆì—­ ì‹¤íŒ¨: SRT íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    print("ğŸ”Š 6. íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ TTS ìƒì„± ì¤‘...")
    translated_audio = generate_tts_with_timestamps(translated_srt, voice_id)

    if translated_audio:
        print("ğŸ¬ 7. ìƒˆë¡œìš´ ìŒì„±ì„ ì›ë³¸ ì˜ìƒì— í•©ì¹˜ê¸°...")
        final_video = merge_audio_with_video(video_file, translated_audio)
        print("âœ… ë³€í™˜ ì™„ë£Œ! ìµœì¢… íŒŒì¼:", final_video)
    else:
        print("âŒ ì˜¤ë¥˜ ë°œìƒ: ìƒˆë¡œìš´ ìŒì„± ìƒì„± ì‹¤íŒ¨.")

process_video(video_url, target_language, voice_id)