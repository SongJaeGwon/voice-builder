import os
import re
import yt_dlp
import json
import subprocess
import requests
import whisperx
import torch
import openai
from elevenlabs.client import ElevenLabs
from pydub import AudioSegment
from dotenv import load_dotenv
from huggingface_hub import login

# ğŸ”¹ .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ğŸ”¹ í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")
DEEPL_API_KEY = os.getenv("DEEPL_API_KEY")
API_URL = os.getenv("API_URL")
HF_TOKEN = os.getenv("HUGGING_FACE_TOKEN")
OPEN_AI = os.getenv("OPEN_AI")

video_url = "https://www.youtube.com/watch?v=cQ0g6RHB4wA"  # ë³€í™˜í•  ìœ íŠœë¸Œ ì˜ìƒ ë§í¬
# target_language = "ZH-HANT"  # ë²ˆì—­í•  ì–¸ì–´ (ì˜ˆ: ì¼ë³¸ì–´)
target_language = "KO"  # ë²ˆì—­í•  ì–¸ì–´ (ì˜ˆ: ì¼ë³¸ì–´)
voice_id = "ir1CeAgkMhxW2txdJpxQ"  # ElevenLabsì—ì„œ í•™ìŠµí•œ ëª©ì†Œë¦¬ ID ì…ë ¥

def download_youtube_video(video_url, output_filename="downloaded_video.mp4"):
    ydl_opts = {
        'format': 'bestvideo+bestaudio/best',
        'outtmpl': output_filename,
        'merge_output_format': 'mp4'
    }
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.download([video_url])
    return output_filename

def trim_video(input_file, output_file="trimmed_video.mp4", start_time="00:00:00", duration="300"):
    command = f"ffmpeg -i {input_file} -ss {start_time} -t {duration} -c:v copy -c:a copy {output_file} -y"
    subprocess.run(command, shell=True)
    return output_file

def extract_audio_from_video(video_file, audio_output="audio.mp3"):
    command = f"ffmpeg -i {video_file} -q:a 0 -map a {audio_output} -y"
    subprocess.run(command, shell=True)
    return audio_output

def separate_background_audio(input_file, output_file="background_audio.mp3"):
    """
    Demucsë¥¼ ì´ìš©í•´ ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ë³´ì»¬ íŠ¸ë™ì„ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜.
    ê¸°ë³¸ì ìœ¼ë¡œ demucsëŠ” ì§€ì •í•œ output_dirì— ë¶„ë¦¬ëœ íŒŒì¼ë“¤ì„ ì €ì¥í•©ë‹ˆë‹¤.

    :param input_file: ì›ë³¸ ì˜¤ë””ì˜¤ íŒŒì¼ (ì˜ˆ: audio.mp3)
    :param output_dir: Demucsê°€ ê²°ê³¼ë¥¼ ì €ì¥í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’ "separated")
    :return: ë¶„ë¦¬ëœ ë³´ì»¬ íŒŒì¼ì˜ ê²½ë¡œ (ì˜ˆ: separated/{base_name}/vocals.wav)
    """
    command = f"demucs --two-stems=vocals --out=. --filename=out.mp3 --mp3 {input_file}"
    subprocess.run(command, shell=True, check=True)

    return output_file

def transcribe_audio_whisper(audio_file, model="whisper-1"):
    """
    OpenAI Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    """
    client = openai.OpenAI(api_key=OPEN_AI)  # ìµœì‹  ë°©ì‹ìœ¼ë¡œ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

    # ğŸ”¹ OpenAI Whisper API ìš”ì²­
    with open(audio_file, "rb") as file:
        response = client.audio.transcriptions.create(
            model=model,
            file=file,
            response_format="verbose_json",  # JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ìš”ì²­
        )

    # ğŸ”¹ JSONìœ¼ë¡œ ë³€í™˜ (í•„ìˆ˜)
    response_json = response.model_dump()  # dict() ëŒ€ì‹  ìµœì‹  ë²„ì „ì—ì„œëŠ” model_dump() ì‚¬ìš©

    # ğŸ”¹ ë³€í™˜ëœ ë°ì´í„° ì €ì¥ (JSON)
    with open("transcription_whisper.json", "w", encoding="utf-8") as f:
        json.dump(response_json, f, indent=4, ensure_ascii=False)

    return response_json

def transcribe_audio_whisperx(audio_file, language="ko", device=None, num_speakers=None):
    """
    WhisperXë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  SRT íŒŒì¼ì„ ìƒì„± (ë‹¤ì¤‘ í™”ì í¬í•¨)
    """

    # ğŸ”¹ ë””ë°”ì´ìŠ¤ ì„¤ì • (Mac í™˜ê²½ì—ì„œëŠ” CPU ë˜ëŠ” MPS ì‚¬ìš©)
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ CUDA, ì—†ìœ¼ë©´ CPU

    # ğŸ”¹ WhisperX ëª¨ë¸ ë¡œë“œ
    model = whisperx.load_model("large-v2", "cpu", compute_type="float32")

    audio = whisperx.load_audio(audio_file)

    # ğŸ”¹ ì˜¤ë””ì˜¤ ë³€í™˜ (STT ìˆ˜í–‰)
    transcription = model.transcribe(audio)

    # ğŸ”¹ Align (ìŒì„±-í…ìŠ¤íŠ¸ ì •ë ¬)
    align_model, align_metadata = whisperx.load_align_model(language_code=transcription["language"], device=device)
    whisper_results = whisperx.align(transcription["segments"], align_model, align_metadata, audio, device, return_char_alignments=False)

    # ğŸ”¥ ë‹¤ì¤‘ í™”ì(Speaker Diarization) ëª¨ë¸ ë¡œë“œ (Hugging Face í† í° í¬í•¨)
    diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)

    # ğŸ”¹ í™”ì ë¶„ë¦¬ ì‹¤í–‰
    diarization_result = diarize_model(audio, num_speakers=num_speakers) if num_speakers else diarize_model(audio)

    # ğŸ”¹ STT ê²°ê³¼ì™€ í™”ì ì •ë³´ ë™ê¸°í™”
    whisper_results = whisperx.assign_word_speakers(diarization_result, whisper_results)

    # ğŸ”¹ ë³€í™˜ëœ ë°ì´í„° ì €ì¥ (JSON)
    with open("transcription_whisperx.json", "w", encoding="utf-8") as f:
        json.dump(whisper_results, f, indent=4, ensure_ascii=False)

    return whisper_results

def seconds_to_srt_time(seconds):
    """ ì´ˆ ë‹¨ìœ„ë¥¼ SRT í˜•ì‹ (HH:MM:SS,mmm)ìœ¼ë¡œ ë³€í™˜ """
    millisec = int((seconds % 1) * 1000)
    seconds = int(seconds)
    hours, remainder = divmod(seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    return f"{hours:02}:{minutes:02}:{seconds:02},{millisec:03}"

def create_srt_v1(transcription, output_srt="output.srt"):
    """
    Whisper ë³€í™˜ ê²°ê³¼ë¥¼ SRT íŒŒì¼ë¡œ ì €ì¥
    """
    if "segments" not in transcription:
        print("âš ï¸ ë³€í™˜ëœ ìë§‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    with open(output_srt, "w", encoding="utf-8") as f:
        for idx, segment in enumerate(transcription["segments"]):
            start_time = seconds_to_srt_time(segment["start"])
            end_time = seconds_to_srt_time(segment["end"])
            text = segment["text"]

            f.write(f"{idx+1}\n{start_time} --> {end_time}\n{text}\n\n")

    print(f"âœ… SRT íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_srt}")

def translate_text(text, src_lang="KO", target_lang="JA"):
    """
    DeepL APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë²ˆì—­
    :param text: ë²ˆì—­í•  ì›ë³¸ í…ìŠ¤íŠ¸
    :param src_lang: ì›ë³¸ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: "KO" -> í•œêµ­ì–´)
    :param target_lang: ëª©í‘œ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: "JA" -> ì¼ë³¸ì–´)
    :return: ë²ˆì—­ëœ í…ìŠ¤íŠ¸
    """
    headers = {"Authorization": f"DeepL-Auth-Key {DEEPL_API_KEY}"}
    data = {
        "text": [text],  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì „ë‹¬í•´ì•¼ í•¨
        "source_lang": src_lang.upper(),  # DeepLì€ ëŒ€ë¬¸ì ì–¸ì–´ ì½”ë“œ ì‚¬ìš©
        "target_lang": target_lang.upper()
    }

    response = requests.post(API_URL, headers=headers, data=data)

    if response.status_code == 200:
        result = response.json()
        return result["translations"][0]["text"]  # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ë°˜í™˜
    else:
        print("âŒ ë²ˆì—­ ì‹¤íŒ¨:", response.text)
        return None

def translate_srt(input_srt, output_srt, src_lang="KO", target_lang="JA"):
    """
    DeepL APIë¥¼ ì‚¬ìš©í•˜ì—¬ SRT íŒŒì¼ì„ ë²ˆì—­í•˜ì—¬ ìƒˆë¡œìš´ SRT íŒŒì¼ ìƒì„±
    """
    with open(input_srt, "r", encoding="utf-8") as f:
        lines = f.readlines()

    translated_lines = []
    for line in lines:
        if "-->" in line or line.strip().isdigit() or line.strip() == "":
            translated_lines.append(line)  # ì‹œê°„ ë° ì¸ë±ìŠ¤ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        else:
            translated_text = translate_text(line.strip(), src_lang, target_lang)
            translated_lines.append(translated_text + "\n")

    with open(output_srt, "w", encoding="utf-8") as f:
        f.writelines(translated_lines)

    print(f"âœ… ë²ˆì—­ ì™„ë£Œ! ìƒˆë¡œìš´ SRT íŒŒì¼ ì €ì¥: {output_srt}")
    return output_srt

# ğŸ”¹ Step 5: ElevenLabs APIë¥¼ ì´ìš©í•´ ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
def generate_speech_with_elevenlabs(text, voice_id, output_audio):
    client = ElevenLabs(api_key=ELEVENLABS_API_KEY)

    # TTS ë³€í™˜ ì‹¤í–‰
    audio = client.text_to_speech.convert(
        voice_id=voice_id,
        output_format="mp3_44100_128",
        text=text,
        model_id="eleven_multilingual_v2"
    )

    # ğŸ”¹ ì œë„ˆë ˆì´í„° ë°ì´í„°ë¥¼ ë°”ì´íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    audio_bytes = b"".join(audio)  

    # ë³€í™˜ëœ ì˜¤ë””ì˜¤ ì €ì¥
    with open(output_audio, "wb") as f:
        f.write(audio_bytes)

def parse_srt(srt_file):
    """
    SRT íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë° í…ìŠ¤íŠ¸ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì¶”ì¶œ
    """
    srt_pattern = re.compile(r"(\d+)\s*\n(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})\s*\n([\s\S]*?)(?=\n\d+\s*\n|\Z)")

    with open(srt_file, "r", encoding="utf-8") as file:
        content = file.read()

    matches = srt_pattern.findall(content)
    subtitles = []

    for _, start, end, text in matches:
        start_seconds = srt_time_to_seconds(start)
        end_seconds = srt_time_to_seconds(end)

        # ğŸ”¹ ì—¬ëŸ¬ ì¤„ì˜ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì¤„ë¡œ í•©ì¹˜ê¸°
        text = text.strip().replace("\n", " ")

        subtitles.append({"start": start_seconds, "end": end_seconds, "text": text})

    return subtitles

def srt_time_to_seconds(time_str):
    """SRT ì‹œê°„ ë¬¸ìì—´ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜ (HH:MM:SS,mmm â†’ ì´ˆ)"""
    hours, minutes, seconds_millis = time_str.split(":")
    seconds, millis = map(int, seconds_millis.split(","))
    return int(hours) * 3600 + int(minutes) * 60 + seconds + millis / 1000

def generate_tts_with_timestamps(srt_file, voice_id, output_audio="final_tts_audio.mp3"):
    """
    ë²ˆì—­ëœ SRT íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ, íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ê³ ë ¤í•œ ìŒì„±ì„ ìƒì„±í•˜ê³  í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë³‘í•©
    """
    subtitles = parse_srt(srt_file)
    
    # ğŸ”¹ ìµœì¢… í•©ì¹  ì˜¤ë””ì˜¤ ë¦¬ìŠ¤íŠ¸
    combined_audio = AudioSegment.silent(duration=0)
    
    for idx, subtitle in enumerate(subtitles):
        text = subtitle["text"]
        start_time = int(subtitle["start"] * 1000)  # ë°€ë¦¬ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
        end_time = int(subtitle["end"] * 1000)  # ì¢…ë£Œ ì‹œê°„ (ms)
        duration = end_time - start_time  # í•´ë‹¹ êµ¬ê°„ ê¸¸ì´ (ms)

        tts_filename = f"temp_{idx}.mp3"

        # ğŸ”¹ ElevenLabs TTS ìƒì„±
        generate_speech_with_elevenlabs(text, voice_id, tts_filename)

        # ğŸ”¹ TTS ì˜¤ë””ì˜¤ ë¡œë“œ
        tts_audio = AudioSegment.from_file(tts_filename)
        tts_duration = len(tts_audio)  # ì‹¤ì œ ìƒì„±ëœ ìŒì„± ê¸¸ì´ (ms)

        # ğŸ”¹ ìŒì„± ê¸¸ì´ ì¡°ì •
        if tts_duration > duration:
            speed_factor = tts_duration / duration  # ì¤„ì—¬ì•¼ í•˜ëŠ” ë°°ì† ë¹„ìœ¨ ê³„ì‚°
            print(f"âš ï¸ ìƒì„±ëœ TTS({tts_duration}ms)ê°€ ì›ë³¸ë³´ë‹¤ ê¹€({duration}ms), {speed_factor:.2f}ë°° ì†ë„ë¡œ ë¹ ë¥´ê²Œ ì¬ìƒ")
            tts_audio = tts_audio.speedup(playback_speed=speed_factor)  # ì†ë„ ì¦ê°€
        elif tts_duration < duration:
            print(f"âš ï¸ ìƒì„±ëœ TTS({tts_duration}ms)ê°€ ì›ë³¸ë³´ë‹¤ ì§§ìŒ({duration}ms), íŒ¨ë”© ì¶”ê°€")
            silence = AudioSegment.silent(duration=duration - tts_duration)  # ì§§ì€ ê²½ìš°, ë¬´ìŒ ì¶”ê°€
            tts_audio = tts_audio + silence

        # ğŸ”¹ ìŒì„± ì‚½ì… (ì¤‘ê°„ì— ê³µë°±ì„ ë„£ì–´ì•¼ í•¨)
        silence_gap = AudioSegment.silent(duration=start_time - len(combined_audio))
        combined_audio = combined_audio + silence_gap + tts_audio

        # ğŸ”¹ ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(tts_filename)

    # ğŸ”¹ ìµœì¢… ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥
    combined_audio.export(output_audio, format="mp3")
    print(f"âœ… íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ê³ ë ¤ëœ ìŒì„± íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_audio}")
    return output_audio

# ğŸ”¹ Step 6: ìƒˆ ìŒì„±ì„ ì›ë³¸ ì˜ìƒì— í•©ì¹˜ê¸°
def merge_audio_with_video(original_video, new_audio, output_video="final_video.mp4"):
    command = f"ffmpeg -i {original_video} -i {new_audio} -c:v copy -map 0:v:0 -map 1:a:0 -shortest {output_video} -y"
    subprocess.run(command, shell=True)
    return output_video

# ğŸ”¹ Step 7: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
def process_video(video_url, target_language="JA", voice_id="YOUR_VOICE_ID"):
    print("ğŸ“¥ 1. ìœ íŠœë¸Œ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    # video_file = download_youtube_video(video_url)

    video_file = "ì •ì—°_ì¤‘êµ­ì–´.mp4"

    print("âœ‚ï¸ FFmpegë¡œ 30ì´ˆ ê¸¸ì´ë¡œ ìë¥´ê¸°...")
    trimmed_video = trim_video(video_file)

    print("ğŸ™ï¸ 2. ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘...")
    audio_file = extract_audio_from_video(trimmed_video)

    # Demucsë¥¼ ì‹¤í–‰í•˜ì—¬ ë°°ê²½ìŒ íŠ¸ë™ ë¶„ë¦¬
    try:
        print("ğŸšï¸ 2-1. Demucsë¡œ ë³´ì»¬ ë¶„ë¦¬ ì¤‘...")
        separate_background_audio(audio_file)
    except Exception as e:
        print("âŒ ë³´ì»¬ ë¶„ë¦¬ ì‹¤íŒ¨:", e)
        return

    print("ğŸ“ 3. ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...")
    original_text = transcribe_audio_whisper(audio_file)

    print("ğŸ“ 4. SRT íŒŒì¼ ìƒì„± ì¤‘...")
    create_srt_v1(original_text, "transcription.srt")

    print(f"ğŸŒ 5. ë²ˆì—­ ì¤‘... (ì–¸ì–´: {target_language})")
    translated_srt = translate_srt("transcription.srt", "translated.srt", src_lang="ZH-HANT", target_lang=target_language)

    if translated_srt is None:
        print("âŒ ë²ˆì—­ ì‹¤íŒ¨: SRT íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    print("ğŸ”Š 6. íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ TTS ìƒì„± ì¤‘...")
    translated_audio = generate_tts_with_timestamps(translated_srt, voice_id)

    # Merge the translated audio with background audio
    print("ğŸµ 6-1. background audio í•©ì¹˜ëŠ” ì¤‘...")
    translated_audio_segment = AudioSegment.from_file(translated_audio)
    background_audio_segment = AudioSegment.from_file("./htdemucs/out.mp3")

    # Ensure both audio segments are the same length
    background_length = len(background_audio_segment)
    translated_length = len(translated_audio_segment)
    if background_length > translated_length:
        background_audio_segment = background_audio_segment[:translated_length]

    final_audio = translated_audio_segment.overlay(background_audio_segment)
    final_audio.export(translated_audio, format="mp3")

    if final_audio:
        print("ğŸ¬ 7. ìƒˆë¡œìš´ ìŒì„±ì„ ì›ë³¸ ì˜ìƒì— í•©ì¹˜ê¸°...")
        final_video = merge_audio_with_video(trimmed_video, translated_audio)
        print("âœ… ë³€í™˜ ì™„ë£Œ! ìµœì¢… íŒŒì¼:", final_video)
    else:
        print("âŒ ì˜¤ë¥˜ ë°œìƒ: ìƒˆë¡œìš´ ìŒì„± ìƒì„± ì‹¤íŒ¨.")

process_video(video_url, target_language, voice_id)